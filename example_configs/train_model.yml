train_model:
  dataset: "roneneldan/TinyStories"           # Required
  model_id: "TinyStories_4096"        # Required
  tokenizer_prefix: "TinyStories_4096"  # Required
  
  # Model architecture parameters
  vocab_size: 4096                
  dim: 64                         
  n_layers: 5                      
  n_heads: 8                      
  max_seq_len: 256                
  
  # Training parameters
  steps: 10000                 
  batch_size: 128           
  learning_rate: 0.001          
  gradient_accumulation_steps: 2  
  chunk_size: 512                
  resume: false    
  model_architecture: llama2     