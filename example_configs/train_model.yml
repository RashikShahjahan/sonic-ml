train_model:
  dataset: "roneneldan/TinyStories"           # Required
  model_id: "TinyStories_4096"        # Required
  tokenizer_prefix: "TinyStories_4096"  # Required
  
  # Model architecture parameters
  vocab_size: 4096                
  dim: 64                         
  n_layers: 6                      
  n_heads: 6                      
  max_seq_len: 256                
  
  # Training parameters
  steps: 10000                 
  batch_size: 8           
  learning_rate: 0.001          
  gradient_accumulation_steps: 8
  chunk_size: 512                
  resume: false    
  model_architecture: llama 